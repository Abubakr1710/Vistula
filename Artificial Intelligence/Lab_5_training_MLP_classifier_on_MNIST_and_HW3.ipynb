{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 14pt;\">Prof. Krzysztof Rybinski</div><br/><br/>\n",
    "<div style=\"font-size: 22pt;\"><b>Artificial Intelligence course</b></div><br/><br/>\n",
    "<div style=\"font-size: 18pt;\">LAB 5.3</div><br/>\n",
    "<div style=\"font-size: 18pt;\">- Predicting handwritten digits in MNIST dataset with MLP</div><br/><br/>\n",
    "<div style=\"font-size: 18pt;\">- Homework 3 described at the end of this Jupyter Notebook</div><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:21:42.701445Z",
     "start_time": "2018-11-13T15:21:39.648435Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 15:55:58.889430: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-20 15:55:59.034137: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-20 15:55:59.034153: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-20 15:55:59.745934: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-20 15:55:59.746034: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-20 15:55:59.746041: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check MNIST data information\n",
    "# https://en.wikipedia.org/wiki/MNIST_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:29:02.781102Z",
     "start_time": "2018-11-13T15:29:02.369623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 7s 1us/step\n"
     ]
    }
   ],
   "source": [
    "dataset = mnist.load_data()\n",
    "\n",
    "(Xtrain, ytrain), (Xtest, ytest) = dataset\n",
    "\n",
    "n_train = len(Xtrain)\n",
    "n_test = len(Xtest)\n",
    "\n",
    "n_features = 28*28\n",
    "\n",
    "Xtrain = Xtrain.reshape( n_train, n_features )\n",
    "Xtest  = Xtest.reshape( n_test, n_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:29:02.781102Z",
     "start_time": "2018-11-13T15:29:02.369623Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow( np.reshape( Xtrain[10002], (28,28) ) , cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:29:02.787949Z",
     "start_time": "2018-11-13T15:29:02.783514Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:29:17.565311Z",
     "start_time": "2018-11-13T15:29:17.559317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:29:32.437992Z",
     "start_time": "2018-11-13T15:29:32.433262Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:45:27.566378Z",
     "start_time": "2018-11-13T15:45:27.562418Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100,), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check info on MLPClassifer in sklearn\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:46:02.148932Z",
     "start_time": "2018-11-13T15:45:28.037088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.15170533\n",
      "Iteration 2, loss = 0.77079898\n",
      "Iteration 3, loss = 0.43117083\n",
      "Iteration 4, loss = 0.30808108\n",
      "Iteration 5, loss = 0.24168570\n",
      "Iteration 6, loss = 0.20230420\n",
      "Iteration 7, loss = 0.17747831\n",
      "Iteration 8, loss = 0.15173311\n",
      "Iteration 9, loss = 0.13501112\n",
      "Iteration 10, loss = 0.12069369\n",
      "Iteration 11, loss = 0.11981354\n",
      "Iteration 12, loss = 0.10958129\n",
      "Iteration 13, loss = 0.10694802\n",
      "Iteration 14, loss = 0.10296149\n",
      "Iteration 15, loss = 0.09563402\n",
      "Iteration 16, loss = 0.09602115\n",
      "Iteration 17, loss = 0.09345409\n",
      "Iteration 18, loss = 0.08903324\n",
      "Iteration 19, loss = 0.07918831\n",
      "Iteration 20, loss = 0.08683447\n",
      "Iteration 21, loss = 0.08151253\n",
      "Iteration 22, loss = 0.07926298\n",
      "Iteration 23, loss = 0.06978583\n",
      "Iteration 24, loss = 0.06742438\n",
      "Iteration 25, loss = 0.06691623\n",
      "Iteration 26, loss = 0.07050255\n",
      "Iteration 27, loss = 0.07106225\n",
      "Iteration 28, loss = 0.06999583\n",
      "Iteration 29, loss = 0.06368613\n",
      "Iteration 30, loss = 0.05978084\n",
      "Iteration 31, loss = 0.05920841\n",
      "Iteration 32, loss = 0.05741084\n",
      "Iteration 33, loss = 0.05970501\n",
      "Iteration 34, loss = 0.05974979\n",
      "Iteration 35, loss = 0.05529028\n",
      "Iteration 36, loss = 0.05486067\n",
      "Iteration 37, loss = 0.05248970\n",
      "Iteration 38, loss = 0.04333583\n",
      "Iteration 39, loss = 0.04410950\n",
      "Iteration 40, loss = 0.04727393\n",
      "Iteration 41, loss = 0.05106446\n",
      "Iteration 42, loss = 0.04388169\n",
      "Iteration 43, loss = 0.04516540\n",
      "Iteration 44, loss = 0.04025959\n",
      "Iteration 45, loss = 0.06178954\n",
      "Iteration 46, loss = 0.04969927\n",
      "Iteration 47, loss = 0.04609152\n",
      "Iteration 48, loss = 0.05204479\n",
      "Iteration 49, loss = 0.04257648\n",
      "Iteration 50, loss = 0.04086283\n",
      "Iteration 51, loss = 0.03884913\n",
      "Iteration 52, loss = 0.04194924\n",
      "Iteration 53, loss = 0.04451956\n",
      "Iteration 54, loss = 0.04463133\n",
      "Iteration 55, loss = 0.03807863\n",
      "Iteration 56, loss = 0.03509490\n",
      "Iteration 57, loss = 0.05148741\n",
      "Iteration 58, loss = 0.04384203\n",
      "Iteration 59, loss = 0.03910764\n",
      "Iteration 60, loss = 0.03459073\n",
      "Iteration 61, loss = 0.03504755\n",
      "Iteration 62, loss = 0.03518317\n",
      "Iteration 63, loss = 0.04382316\n",
      "Iteration 64, loss = 0.05163719\n",
      "Iteration 65, loss = 0.03669439\n",
      "Iteration 66, loss = 0.03952190\n",
      "Iteration 67, loss = 0.03988500\n",
      "Iteration 68, loss = 0.03433557\n",
      "Iteration 69, loss = 0.02788635\n",
      "Iteration 70, loss = 0.03097179\n",
      "Iteration 71, loss = 0.05171436\n",
      "Iteration 72, loss = 0.04567194\n",
      "Iteration 73, loss = 0.04224296\n",
      "Iteration 74, loss = 0.02599508\n",
      "Iteration 75, loss = 0.03168363\n",
      "Iteration 76, loss = 0.03407918\n",
      "Iteration 77, loss = 0.02983597\n",
      "Iteration 78, loss = 0.03008149\n",
      "Iteration 79, loss = 0.03696034\n",
      "Iteration 80, loss = 0.04091765\n",
      "Iteration 81, loss = 0.03562405\n",
      "Iteration 82, loss = 0.03375580\n",
      "Iteration 83, loss = 0.02832162\n",
      "Iteration 84, loss = 0.03581279\n",
      "Iteration 85, loss = 0.03413944\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(verbose=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on MLPClassifier in module sklearn.neural_network._multilayer_perceptron object:\n",
      "\n",
      "class MLPClassifier(sklearn.base.ClassifierMixin, BaseMultilayerPerceptron)\n",
      " |  MLPClassifier(hidden_layer_sizes=(100,), activation='relu', *, solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)\n",
      " |  \n",
      " |  Multi-layer Perceptron classifier.\n",
      " |  \n",
      " |  This model optimizes the log-loss function using LBFGS or stochastic\n",
      " |  gradient descent.\n",
      " |  \n",
      " |  .. versionadded:: 0.18\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  hidden_layer_sizes : array-like of shape(n_layers - 2,), default=(100,)\n",
      " |      The ith element represents the number of neurons in the ith\n",
      " |      hidden layer.\n",
      " |  \n",
      " |  activation : {'identity', 'logistic', 'tanh', 'relu'}, default='relu'\n",
      " |      Activation function for the hidden layer.\n",
      " |  \n",
      " |      - 'identity', no-op activation, useful to implement linear bottleneck,\n",
      " |        returns f(x) = x\n",
      " |  \n",
      " |      - 'logistic', the logistic sigmoid function,\n",
      " |        returns f(x) = 1 / (1 + exp(-x)).\n",
      " |  \n",
      " |      - 'tanh', the hyperbolic tan function,\n",
      " |        returns f(x) = tanh(x).\n",
      " |  \n",
      " |      - 'relu', the rectified linear unit function,\n",
      " |        returns f(x) = max(0, x)\n",
      " |  \n",
      " |  solver : {'lbfgs', 'sgd', 'adam'}, default='adam'\n",
      " |      The solver for weight optimization.\n",
      " |  \n",
      " |      - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n",
      " |  \n",
      " |      - 'sgd' refers to stochastic gradient descent.\n",
      " |  \n",
      " |      - 'adam' refers to a stochastic gradient-based optimizer proposed\n",
      " |        by Kingma, Diederik, and Jimmy Ba\n",
      " |  \n",
      " |      Note: The default solver 'adam' works pretty well on relatively\n",
      " |      large datasets (with thousands of training samples or more) in terms of\n",
      " |      both training time and validation score.\n",
      " |      For small datasets, however, 'lbfgs' can converge faster and perform\n",
      " |      better.\n",
      " |  \n",
      " |  alpha : float, default=0.0001\n",
      " |      Strength of the L2 regularization term. The L2 regularization term\n",
      " |      is divided by the sample size when added to the loss.\n",
      " |  \n",
      " |  batch_size : int, default='auto'\n",
      " |      Size of minibatches for stochastic optimizers.\n",
      " |      If the solver is 'lbfgs', the classifier will not use minibatch.\n",
      " |      When set to \"auto\", `batch_size=min(200, n_samples)`.\n",
      " |  \n",
      " |  learning_rate : {'constant', 'invscaling', 'adaptive'}, default='constant'\n",
      " |      Learning rate schedule for weight updates.\n",
      " |  \n",
      " |      - 'constant' is a constant learning rate given by\n",
      " |        'learning_rate_init'.\n",
      " |  \n",
      " |      - 'invscaling' gradually decreases the learning rate at each\n",
      " |        time step 't' using an inverse scaling exponent of 'power_t'.\n",
      " |        effective_learning_rate = learning_rate_init / pow(t, power_t)\n",
      " |  \n",
      " |      - 'adaptive' keeps the learning rate constant to\n",
      " |        'learning_rate_init' as long as training loss keeps decreasing.\n",
      " |        Each time two consecutive epochs fail to decrease training loss by at\n",
      " |        least tol, or fail to increase validation score by at least tol if\n",
      " |        'early_stopping' is on, the current learning rate is divided by 5.\n",
      " |  \n",
      " |      Only used when ``solver='sgd'``.\n",
      " |  \n",
      " |  learning_rate_init : float, default=0.001\n",
      " |      The initial learning rate used. It controls the step-size\n",
      " |      in updating the weights. Only used when solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  power_t : float, default=0.5\n",
      " |      The exponent for inverse scaling learning rate.\n",
      " |      It is used in updating effective learning rate when the learning_rate\n",
      " |      is set to 'invscaling'. Only used when solver='sgd'.\n",
      " |  \n",
      " |  max_iter : int, default=200\n",
      " |      Maximum number of iterations. The solver iterates until convergence\n",
      " |      (determined by 'tol') or this number of iterations. For stochastic\n",
      " |      solvers ('sgd', 'adam'), note that this determines the number of epochs\n",
      " |      (how many times each data point will be used), not the number of\n",
      " |      gradient steps.\n",
      " |  \n",
      " |  shuffle : bool, default=True\n",
      " |      Whether to shuffle samples in each iteration. Only used when\n",
      " |      solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Determines random number generation for weights and bias\n",
      " |      initialization, train-test split if early stopping is used, and batch\n",
      " |      sampling when solver='sgd' or 'adam'.\n",
      " |      Pass an int for reproducible results across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for the optimization. When the loss or score is not improving\n",
      " |      by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,\n",
      " |      unless ``learning_rate`` is set to 'adaptive', convergence is\n",
      " |      considered to be reached and training stops.\n",
      " |  \n",
      " |  verbose : bool, default=False\n",
      " |      Whether to print progress messages to stdout.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to True, reuse the solution of the previous\n",
      " |      call to fit as initialization, otherwise, just erase the\n",
      " |      previous solution. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  momentum : float, default=0.9\n",
      " |      Momentum for gradient descent update. Should be between 0 and 1. Only\n",
      " |      used when solver='sgd'.\n",
      " |  \n",
      " |  nesterovs_momentum : bool, default=True\n",
      " |      Whether to use Nesterov's momentum. Only used when solver='sgd' and\n",
      " |      momentum > 0.\n",
      " |  \n",
      " |  early_stopping : bool, default=False\n",
      " |      Whether to use early stopping to terminate training when validation\n",
      " |      score is not improving. If set to true, it will automatically set\n",
      " |      aside 10% of training data as validation and terminate training when\n",
      " |      validation score is not improving by at least tol for\n",
      " |      ``n_iter_no_change`` consecutive epochs. The split is stratified,\n",
      " |      except in a multilabel setting.\n",
      " |      If early stopping is False, then the training stops when the training\n",
      " |      loss does not improve by more than tol for n_iter_no_change consecutive\n",
      " |      passes over the training set.\n",
      " |      Only effective when solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  validation_fraction : float, default=0.1\n",
      " |      The proportion of training data to set aside as validation set for\n",
      " |      early stopping. Must be between 0 and 1.\n",
      " |      Only used if early_stopping is True.\n",
      " |  \n",
      " |  beta_1 : float, default=0.9\n",
      " |      Exponential decay rate for estimates of first moment vector in adam,\n",
      " |      should be in [0, 1). Only used when solver='adam'.\n",
      " |  \n",
      " |  beta_2 : float, default=0.999\n",
      " |      Exponential decay rate for estimates of second moment vector in adam,\n",
      " |      should be in [0, 1). Only used when solver='adam'.\n",
      " |  \n",
      " |  epsilon : float, default=1e-8\n",
      " |      Value for numerical stability in adam. Only used when solver='adam'.\n",
      " |  \n",
      " |  n_iter_no_change : int, default=10\n",
      " |      Maximum number of epochs to not meet ``tol`` improvement.\n",
      " |      Only effective when solver='sgd' or 'adam'.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  max_fun : int, default=15000\n",
      " |      Only used when solver='lbfgs'. Maximum number of loss function calls.\n",
      " |      The solver iterates until convergence (determined by 'tol'), number\n",
      " |      of iterations reaches max_iter, or this number of loss function calls.\n",
      " |      Note that number of loss function calls will be greater than or equal\n",
      " |      to the number of iterations for the `MLPClassifier`.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : ndarray or list of ndarray of shape (n_classes,)\n",
      " |      Class labels for each output.\n",
      " |  \n",
      " |  loss_ : float\n",
      " |      The current loss computed with the loss function.\n",
      " |  \n",
      " |  best_loss_ : float or None\n",
      " |      The minimum loss reached by the solver throughout fitting.\n",
      " |      If `early_stopping=True`, this attribute is set ot `None`. Refer to\n",
      " |      the `best_validation_score_` fitted attribute instead.\n",
      " |  \n",
      " |  loss_curve_ : list of shape (`n_iter_`,)\n",
      " |      The ith element in the list represents the loss at the ith iteration.\n",
      " |  \n",
      " |  validation_scores_ : list of shape (`n_iter_`,) or None\n",
      " |      The score at each iteration on a held-out validation set. The score\n",
      " |      reported is the accuracy score. Only available if `early_stopping=True`,\n",
      " |      otherwise the attribute is set to `None`.\n",
      " |  \n",
      " |  best_validation_score_ : float or None\n",
      " |      The best validation score (i.e. accuracy score) that triggered the\n",
      " |      early stopping. Only available if `early_stopping=True`, otherwise the\n",
      " |      attribute is set to `None`.\n",
      " |  \n",
      " |  t_ : int\n",
      " |      The number of training samples seen by the solver during fitting.\n",
      " |  \n",
      " |  coefs_ : list of shape (n_layers - 1,)\n",
      " |      The ith element in the list represents the weight matrix corresponding\n",
      " |      to layer i.\n",
      " |  \n",
      " |  intercepts_ : list of shape (n_layers - 1,)\n",
      " |      The ith element in the list represents the bias vector corresponding to\n",
      " |      layer i + 1.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_iter_ : int\n",
      " |      The number of iterations the solver has run.\n",
      " |  \n",
      " |  n_layers_ : int\n",
      " |      Number of layers.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      Number of outputs.\n",
      " |  \n",
      " |  out_activation_ : str\n",
      " |      Name of the output activation function.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  MLPRegressor : Multi-layer Perceptron regressor.\n",
      " |  BernoulliRBM : Bernoulli Restricted Boltzmann Machine (RBM).\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  MLPClassifier trains iteratively since at each time step\n",
      " |  the partial derivatives of the loss function with respect to the model\n",
      " |  parameters are computed to update the parameters.\n",
      " |  \n",
      " |  It can also have a regularization term added to the loss function\n",
      " |  that shrinks model parameters to prevent overfitting.\n",
      " |  \n",
      " |  This implementation works with data represented as dense numpy arrays or\n",
      " |  sparse scipy arrays of floating point values.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  Hinton, Geoffrey E. \"Connectionist learning procedures.\"\n",
      " |  Artificial intelligence 40.1 (1989): 185-234.\n",
      " |  \n",
      " |  Glorot, Xavier, and Yoshua Bengio.\n",
      " |  \"Understanding the difficulty of training deep feedforward neural networks.\"\n",
      " |  International Conference on Artificial Intelligence and Statistics. 2010.\n",
      " |  \n",
      " |  :arxiv:`He, Kaiming, et al (2015). \"Delving deep into rectifiers:\n",
      " |  Surpassing human-level performance on imagenet classification.\" <1502.01852>`\n",
      " |  \n",
      " |  :arxiv:`Kingma, Diederik, and Jimmy Ba (2014)\n",
      " |  \"Adam: A method for stochastic optimization.\" <1412.6980>`\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.neural_network import MLPClassifier\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> from sklearn.model_selection import train_test_split\n",
      " |  >>> X, y = make_classification(n_samples=100, random_state=1)\n",
      " |  >>> X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
      " |  ...                                                     random_state=1)\n",
      " |  >>> clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\n",
      " |  >>> clf.predict_proba(X_test[:1])\n",
      " |  array([[0.038..., 0.961...]])\n",
      " |  >>> clf.predict(X_test[:5, :])\n",
      " |  array([1, 0, 1, 0, 1])\n",
      " |  >>> clf.score(X_test, y_test)\n",
      " |  0.8...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MLPClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseMultilayerPerceptron\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, hidden_layer_sizes=(100,), activation='relu', *, solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  partial_fit(self, X, y, classes=None)\n",
      " |      Update the model with a single iteration over the given data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          The target values.\n",
      " |      \n",
      " |      classes : array of shape (n_classes,), default=None\n",
      " |          Classes across all calls to partial_fit.\n",
      " |          Can be obtained via `np.unique(y_all)`, where y_all is the\n",
      " |          target vector of the entire dataset.\n",
      " |          This argument is required for the first call to partial_fit\n",
      " |          and can be omitted in the subsequent calls.\n",
      " |          Note that y doesn't need to contain all labels in `classes`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Trained MLP model.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the multi-layer perceptron classifier.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray, shape (n_samples,) or (n_samples, n_classes)\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Return the log of probability estimates.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : ndarray of shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      log_y_prob : ndarray of shape (n_samples, n_classes)\n",
      " |          The predicted log-probability of the sample for each class\n",
      " |          in the model, where classes are ordered as they are in\n",
      " |          `self.classes_`. Equivalent to `log(predict_proba(X))`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Probability estimates.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_prob : ndarray of shape (n_samples, n_classes)\n",
      " |          The predicted probability of the sample for each class in the\n",
      " |          model, where classes are ordered as they are in `self.classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseMultilayerPerceptron:\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Fit the model to data matrix X and target(s) y.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : ndarray or sparse matrix of shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns a trained MLP model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from BaseMultilayerPerceptron:\n",
      " |  \n",
      " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-1.37425107e-315,  4.74346844e-316,  3.75118818e-316, ...,\n",
      "         8.15691532e-316, -1.64083567e-315,  1.55577789e-315],\n",
      "       [ 3.56044276e-315,  3.53393146e-316, -4.30498403e-315, ...,\n",
      "         2.31122069e-315,  1.59858421e-315,  1.40751270e-315],\n",
      "       [ 6.45563030e-316,  4.16199465e-315, -2.70478545e-315, ...,\n",
      "        -1.33257153e-316,  3.31248644e-315,  1.73834706e-315],\n",
      "       ...,\n",
      "       [-1.68332998e-316, -8.09886058e-316, -1.17592026e-316, ...,\n",
      "         1.19233038e-315,  1.85987990e-315, -2.39591023e-315],\n",
      "       [ 4.03978296e-315,  1.93310544e-315,  2.16649322e-316, ...,\n",
      "        -4.08026292e-316, -3.75235818e-316,  2.59059549e-315],\n",
      "       [-2.44383430e-315, -4.18936335e-315, -2.74493171e-315, ...,\n",
      "         1.71721651e-315,  2.50319110e-315,  6.94256559e-316]]), array([[ 1.18038058e-01, -9.88583397e-02, -6.04006091e-02,\n",
      "         8.74591325e-02,  6.98126062e-02, -1.48376502e-02,\n",
      "        -5.37803924e-02, -8.56127770e-02,  8.75048346e-03,\n",
      "         3.59811899e-02],\n",
      "       [ 5.27485066e-02, -1.81145717e-02,  1.38930840e-01,\n",
      "         1.55119027e-02, -1.42046938e-01, -2.66599206e-02,\n",
      "        -1.23272038e-01,  6.30941094e-02,  2.66034382e-02,\n",
      "        -6.60235200e-02],\n",
      "       [ 1.93323953e-02, -7.60145232e-02, -1.09900419e-02,\n",
      "        -1.46223647e-02, -4.84119937e-02, -3.29834790e-02,\n",
      "        -8.39749755e-02, -5.80887070e-02,  7.71660479e-02,\n",
      "        -1.52665505e-01],\n",
      "       [-5.49825207e-02, -3.91357275e-02,  8.21559908e-02,\n",
      "         2.06638978e-02,  2.84612997e-02,  1.86136866e-01,\n",
      "         9.49942727e-02,  1.39721369e-01,  1.13104275e-01,\n",
      "         1.17787715e-01],\n",
      "       [-9.17615389e-02, -1.70679606e-01, -1.30683846e-01,\n",
      "        -1.61351975e-01, -3.58439247e-02, -3.08684241e-02,\n",
      "         5.29949073e-03, -1.35656331e-01, -5.20724429e-02,\n",
      "        -1.40677114e-01],\n",
      "       [ 1.02232366e-01,  7.51833300e-02,  1.75162503e-01,\n",
      "         2.03531138e-01,  8.13890726e-02,  9.69122809e-02,\n",
      "         8.48533632e-02,  1.39813548e-01,  1.27717756e-01,\n",
      "         2.13672181e-02],\n",
      "       [ 8.38774442e-02,  1.63635062e-02,  1.62358610e-01,\n",
      "         2.52654355e-02, -7.20266708e-03, -1.03618606e-01,\n",
      "         2.12164424e-02, -1.20561447e-01,  1.93874384e-01,\n",
      "         5.03004212e-03],\n",
      "       [ 9.26475546e-02,  1.37554756e-02,  2.71759571e-02,\n",
      "         5.70640836e-02,  4.47263048e-02,  9.32447846e-03,\n",
      "        -9.17856862e-03,  9.20400084e-02,  8.02790403e-02,\n",
      "         4.63665178e-02],\n",
      "       [ 4.41714084e-02,  9.81478085e-03,  8.10090279e-02,\n",
      "         8.50869853e-03,  5.56649732e-02,  3.50230197e-02,\n",
      "        -3.90828513e-02, -1.65052975e-02,  2.91469471e-02,\n",
      "         9.40169625e-02],\n",
      "       [ 5.08450774e-02,  2.22323931e-01,  5.05355629e-02,\n",
      "         5.39372424e-02,  1.67072038e-01,  1.98622777e-01,\n",
      "        -3.22003082e-02,  1.55040188e-01,  9.67259975e-02,\n",
      "         1.42573009e-01],\n",
      "       [-4.59721966e-02, -2.77743804e-02,  2.18121306e-02,\n",
      "         2.13376668e-01, -1.26937842e-01,  1.58263263e-01,\n",
      "         1.44232803e-01, -4.06500432e-02, -1.06074606e-01,\n",
      "        -5.79962500e-02],\n",
      "       [-1.24345214e-01, -2.35407392e-02, -1.34354565e-02,\n",
      "         1.58014618e-01, -6.62993743e-03, -4.13848163e-02,\n",
      "         1.10104744e-01,  2.06316414e-02, -1.68306381e-01,\n",
      "        -2.01931526e-01],\n",
      "       [-2.86526786e-02, -3.48224619e-02, -5.49950453e-02,\n",
      "        -5.37764055e-02, -8.61510181e-02,  1.63905662e-01,\n",
      "        -1.20427184e-01, -5.51372055e-02, -2.57974372e-02,\n",
      "        -1.49934786e-01],\n",
      "       [-2.06249188e-01,  2.10973645e-01,  7.55739514e-02,\n",
      "        -1.34531932e-01, -1.45583103e-01,  7.87487927e-02,\n",
      "        -6.37103639e-02,  2.06920270e-01, -4.02889321e-03,\n",
      "         2.59435753e-02],\n",
      "       [ 3.64298887e-02, -4.19685319e-02, -1.53185973e-01,\n",
      "         3.70894736e-02, -9.31073233e-03,  1.35169478e-01,\n",
      "        -1.25448981e-02, -9.62713623e-03, -9.33733592e-03,\n",
      "         7.53247854e-02],\n",
      "       [ 6.19861049e-03, -1.69118685e-02,  7.84656183e-03,\n",
      "        -7.99171073e-02, -8.06727928e-02, -7.96449513e-02,\n",
      "        -2.17449428e-03, -2.04843265e-01, -4.35922311e-03,\n",
      "        -4.59089154e-02],\n",
      "       [ 4.30777255e-03, -1.91996691e-02,  2.03491437e-02,\n",
      "        -1.73680324e-02, -4.43632520e-02, -1.32654039e-01,\n",
      "         8.77574264e-02, -1.37128286e-01, -6.13966311e-02,\n",
      "        -1.47540021e-01],\n",
      "       [-2.77635291e-03,  2.61614445e-01,  4.33926022e-02,\n",
      "        -2.24164990e-01,  5.60030532e-02, -1.41148814e-01,\n",
      "         1.65210847e-02, -1.20666870e-01,  6.14183159e-02,\n",
      "        -1.28344569e-01],\n",
      "       [-8.87675901e-02,  7.96478260e-02,  6.47119786e-02,\n",
      "         2.58824863e-02, -5.23281689e-02, -1.71996690e-02,\n",
      "        -6.13555127e-02,  6.68704833e-02, -1.14605649e-01,\n",
      "        -9.93477447e-02],\n",
      "       [ 8.81802679e-03,  2.08495016e-03,  9.16367885e-02,\n",
      "         2.64892892e-02, -3.16659716e-02, -2.41816613e-02,\n",
      "         6.52241091e-03, -2.81150471e-02,  1.65899832e-02,\n",
      "         3.09601439e-02],\n",
      "       [ 2.86835999e-02, -1.31044175e-01, -7.83593768e-02,\n",
      "        -3.93397902e-02, -7.46292739e-02,  7.79253369e-02,\n",
      "         1.68857689e-02,  3.32973617e-02, -7.15127478e-02,\n",
      "        -2.18868134e-02],\n",
      "       [-1.82119820e-02, -1.61956109e-01,  2.28787915e-02,\n",
      "         4.68249980e-03, -1.56696425e-01,  1.08135581e-01,\n",
      "         1.55627733e-02, -1.29431622e-02, -7.08656150e-03,\n",
      "        -8.65528109e-02],\n",
      "       [ 1.05848143e-01, -1.42631530e-02,  1.64761522e-01,\n",
      "        -9.68146038e-02, -1.44008887e-02, -1.13529802e-01,\n",
      "         5.95015617e-02,  1.17209611e-02, -7.87460425e-02,\n",
      "         1.91073351e-01],\n",
      "       [ 2.00625326e-01,  4.69370008e-02, -8.27318324e-02,\n",
      "        -4.66266023e-02, -5.35070167e-02, -1.33616477e-01,\n",
      "         4.77555501e-02, -6.81947862e-02, -9.00863773e-02,\n",
      "         1.68147215e-01],\n",
      "       [-7.52673344e-02,  8.14087933e-02,  1.15023589e-02,\n",
      "        -4.15599409e-02,  1.74177605e-01,  1.22516345e-01,\n",
      "        -2.49166952e-02,  1.45877850e-01,  2.19806993e-02,\n",
      "         5.47504492e-02],\n",
      "       [-8.81573099e-02, -5.36714902e-02, -1.50354714e-01,\n",
      "         1.15557315e-01,  7.45392621e-03,  1.29906718e-01,\n",
      "         2.73049901e-03, -6.50842523e-02, -5.28507848e-02,\n",
      "        -9.29276270e-02],\n",
      "       [-3.58414065e-02,  4.17399017e-02,  3.75194463e-02,\n",
      "         1.48090687e-02,  7.48080411e-02, -5.91199217e-03,\n",
      "        -5.04881007e-02,  2.13188185e-04, -2.53207747e-02,\n",
      "        -1.75498634e-02],\n",
      "       [ 1.08132854e-01, -7.12567657e-02,  8.36173530e-02,\n",
      "         3.93252400e-02,  1.36095045e-01,  9.27445447e-02,\n",
      "         3.50780344e-02,  1.41638534e-01,  7.49096699e-02,\n",
      "         1.43657498e-01],\n",
      "       [ 1.80368103e-01,  9.92701586e-02,  1.95442915e-01,\n",
      "        -6.12938247e-02, -2.19977689e-02, -2.54715841e-02,\n",
      "        -2.67230818e-02, -7.03963926e-04,  6.11965877e-03,\n",
      "        -8.45247721e-03],\n",
      "       [-2.02054127e-02,  7.39573139e-02,  9.44841615e-02,\n",
      "         1.23602467e-01,  6.37476211e-02,  8.78221175e-02,\n",
      "         3.42336004e-02,  4.78642147e-02,  1.42643321e-01,\n",
      "         9.66422288e-02],\n",
      "       [ 8.21360863e-02,  1.46001055e-02,  4.67258029e-02,\n",
      "         3.97031648e-02,  3.14124988e-02, -1.47678050e-02,\n",
      "        -1.06013622e-01,  8.43720816e-02,  9.03265231e-02,\n",
      "         1.32348696e-01],\n",
      "       [ 7.54713713e-02,  8.90197872e-02,  1.96185168e-01,\n",
      "         6.34775323e-02,  8.57111789e-02,  1.54772381e-02,\n",
      "        -3.02550318e-02,  6.56213297e-02,  9.73969404e-02,\n",
      "         5.37828358e-02],\n",
      "       [-6.66653022e-02,  1.35001735e-01,  1.59828678e-01,\n",
      "         2.86736553e-02,  5.01614649e-02,  4.47398229e-02,\n",
      "         4.61928062e-02,  1.41483372e-01,  5.13984783e-02,\n",
      "        -8.15978591e-03],\n",
      "       [-7.21764680e-02, -1.31559138e-01, -4.25658784e-02,\n",
      "        -1.54192294e-02, -1.61331318e-01,  3.76275784e-02,\n",
      "         9.21792227e-02,  4.90990088e-02,  7.82054678e-02,\n",
      "        -6.73799398e-03],\n",
      "       [ 6.11084885e-02, -1.48573096e-13, -3.72634493e-03,\n",
      "        -1.85977266e-02, -1.14505627e-08,  2.98096149e-03,\n",
      "        -1.94391553e-02,  2.27595921e-11, -3.29885036e-06,\n",
      "        -4.70533552e-07],\n",
      "       [-1.35849305e-03,  3.71711936e-02,  9.28059346e-02,\n",
      "         8.92867638e-02,  2.02107365e-03, -5.45457328e-02,\n",
      "        -1.17571683e-02, -1.52364027e-01, -3.16474924e-02,\n",
      "        -4.22719161e-04],\n",
      "       [ 3.26160764e-02, -1.13205159e-01,  2.58225257e-03,\n",
      "         8.54064825e-02, -1.46056665e-01,  6.86074879e-02,\n",
      "        -1.07508933e-01, -9.04976953e-02, -6.34501417e-02,\n",
      "        -1.26550886e-01],\n",
      "       [ 1.85795866e-01, -2.22156673e-01, -1.03873661e-01,\n",
      "        -2.96342047e-02,  5.27816955e-02,  8.91857046e-02,\n",
      "         1.26710500e-01,  1.78660010e-02,  1.61739772e-02,\n",
      "         1.50221298e-01],\n",
      "       [-1.34353926e-02, -2.50237476e-02, -3.33730545e-02,\n",
      "         3.14837850e-02,  2.47886839e-02,  2.91921074e-02,\n",
      "         3.50418576e-02, -5.15340214e-02, -1.77820042e-02,\n",
      "        -4.06738174e-02],\n",
      "       [-1.79487120e-01,  1.25930975e-01, -1.20918734e-01,\n",
      "         9.00655077e-03, -2.79672950e-02,  1.40787603e-01,\n",
      "         1.01812288e-02,  2.00777357e-02, -1.14871414e-03,\n",
      "        -1.58958626e-02],\n",
      "       [ 1.40975602e-01, -2.87671779e-04,  1.51695856e-02,\n",
      "        -4.65817414e-03,  1.93480820e-02, -1.81689697e-03,\n",
      "        -1.81112506e-01, -1.35166494e-03, -7.33741959e-03,\n",
      "        -1.52512817e-01],\n",
      "       [-1.66160924e-01, -7.76161372e-02, -1.48166282e-01,\n",
      "        -1.27725775e-01, -2.39071618e-02, -1.25136443e-01,\n",
      "        -1.45903472e-01, -1.34797602e-01, -9.36100564e-02,\n",
      "        -1.92139915e-01],\n",
      "       [ 2.40950824e-02,  7.06581505e-03,  4.95281135e-02,\n",
      "        -6.91669941e-03,  4.94736544e-02, -3.03122636e-02,\n",
      "        -5.39790065e-02, -6.63494140e-02, -5.00255174e-02,\n",
      "        -6.70583443e-02],\n",
      "       [ 1.43195895e-03,  1.93950636e-07, -2.27435721e-04,\n",
      "         7.83797398e-02,  3.34836765e-02, -1.10193545e-02,\n",
      "        -1.47926926e-06, -4.60900348e-03,  1.27905172e-06,\n",
      "        -6.09633573e-06],\n",
      "       [-3.51690726e-02, -3.30914314e-02, -5.48587479e-02,\n",
      "        -6.10855286e-02,  6.93058821e-02,  1.42275172e-02,\n",
      "        -1.43971950e-01, -7.31684230e-02, -1.16262746e-01,\n",
      "         7.31604754e-02],\n",
      "       [ 1.55492856e-01,  1.13127890e-01, -7.86840140e-02,\n",
      "         5.03322654e-02, -3.64681283e-02,  9.86731797e-02,\n",
      "         1.35524922e-01, -7.26476340e-02, -2.98213830e-02,\n",
      "        -1.12462829e-01],\n",
      "       [-6.71681304e-02,  3.19143038e-02,  2.02458373e-01,\n",
      "         2.82469009e-02, -5.91636845e-02, -8.17198031e-02,\n",
      "         4.15066700e-02,  2.29707112e-01,  7.25230217e-02,\n",
      "         3.29684726e-02],\n",
      "       [ 1.52991865e-01,  4.66609724e-04,  1.18885305e-01,\n",
      "        -3.37203686e-02, -1.31275181e-02, -2.82274716e-02,\n",
      "         1.85577300e-02,  9.38687610e-02,  6.28011222e-02,\n",
      "         9.86193015e-02],\n",
      "       [-9.74628720e-02, -8.34508487e-02,  1.46349327e-02,\n",
      "         9.41952203e-02, -3.47184255e-02,  1.02229975e-01,\n",
      "        -6.88736093e-02, -1.03212930e-01,  3.00369550e-02,\n",
      "         2.03919027e-03],\n",
      "       [ 1.59010048e-01, -7.36059123e-02, -6.61739985e-02,\n",
      "         6.48877803e-02,  2.07700949e-02,  5.06026656e-02,\n",
      "        -1.71201466e-02,  2.20172592e-02,  9.02429569e-02,\n",
      "         1.73429920e-01],\n",
      "       [ 7.12802321e-02, -2.16348563e-01, -3.96215027e-02,\n",
      "         2.12228881e-02, -2.44318676e-02,  4.32412298e-02,\n",
      "         1.00768605e-02, -3.28316522e-02, -1.43729616e-02,\n",
      "        -1.90690882e-02],\n",
      "       [ 3.66097724e-02, -6.70804180e-03, -1.06066146e-01,\n",
      "         2.52506311e-03, -7.96596844e-03,  1.40426498e-01,\n",
      "        -1.27541866e-01,  1.95324511e-01,  1.02112498e-01,\n",
      "         1.04121397e-01],\n",
      "       [-7.41009238e-02, -2.94553352e-02,  6.73205849e-02,\n",
      "         4.93764469e-02,  1.53160749e-01,  4.85013755e-02,\n",
      "        -3.94707104e-02,  8.94664558e-02,  8.80844604e-02,\n",
      "         1.49570184e-01],\n",
      "       [-1.40524872e-01, -7.14347733e-03, -1.37777853e-01,\n",
      "        -1.52275586e-01, -1.17838726e-01, -8.02192945e-02,\n",
      "        -2.54023434e-02, -2.00121983e-01, -9.32556778e-02,\n",
      "        -7.02050708e-02],\n",
      "       [-1.07949466e-01,  8.17206088e-02,  7.48670266e-02,\n",
      "         1.85360564e-01,  5.76056168e-02,  1.05424439e-01,\n",
      "        -2.66420921e-02, -1.11900746e-02, -7.12048569e-02,\n",
      "        -3.61541478e-02],\n",
      "       [ 8.33070531e-04, -3.14381584e-05,  2.16370427e-03,\n",
      "        -2.92799373e-04, -3.24792420e-03,  1.72520231e-01,\n",
      "         1.51463749e-04, -5.54455949e-02, -1.86971829e-03,\n",
      "         1.31003465e-01],\n",
      "       [ 5.32007353e-02, -1.87137722e-02,  3.15702486e-02,\n",
      "         1.22809013e-01,  1.16579897e-01,  1.32251950e-01,\n",
      "         5.30450406e-02,  5.76795422e-02,  1.45606796e-01,\n",
      "         1.39404347e-01],\n",
      "       [ 2.92851167e-02,  3.38135776e-02,  7.53929093e-02,\n",
      "         1.05208208e-02, -6.92760036e-02,  1.58282074e-02,\n",
      "         1.52400031e-02, -2.96666472e-02, -8.23175064e-02,\n",
      "        -1.12417801e-02],\n",
      "       [ 4.15362171e-03, -1.27404862e-01,  6.98914390e-02,\n",
      "         4.46539053e-03, -1.45444872e-01,  4.36748727e-02,\n",
      "        -1.01539907e-01, -1.84505555e-05,  3.65346867e-02,\n",
      "         3.46439031e-03],\n",
      "       [-9.78605181e-02, -2.16688039e-01,  4.17002525e-02,\n",
      "         6.91465155e-02,  1.27751244e-01,  1.64856591e-02,\n",
      "         8.88190601e-03,  1.10087065e-01,  6.90545194e-03,\n",
      "         1.38011742e-01],\n",
      "       [ 1.30838855e-01, -1.84007362e-02,  2.92878796e-04,\n",
      "        -5.07726310e-02,  2.17816790e-02,  5.42335022e-02,\n",
      "         7.25972542e-02,  1.14777687e-01,  5.15096334e-02,\n",
      "        -1.64882395e-02],\n",
      "       [ 1.25352988e-01, -7.67175848e-02,  1.48309469e-01,\n",
      "         1.18047762e-01, -3.86062698e-02,  1.07845607e-01,\n",
      "        -4.74673082e-02,  1.57042128e-01,  3.21377147e-02,\n",
      "         4.23637841e-02],\n",
      "       [-1.56880812e-03,  3.42011105e-02,  8.16617620e-02,\n",
      "        -2.64866420e-02,  1.22881013e-01,  5.31001951e-02,\n",
      "         5.71215552e-02, -4.04115150e-02, -9.40980066e-02,\n",
      "         1.79204904e-03],\n",
      "       [ 1.05828306e-01, -3.37703068e-02, -5.00865112e-02,\n",
      "         1.26529842e-01, -4.18780173e-02, -1.28259545e-01,\n",
      "         6.10852794e-02, -1.67778351e-02, -1.41057270e-04,\n",
      "        -6.22699922e-02],\n",
      "       [-9.25094045e-02,  1.43212777e-01,  7.17268379e-02,\n",
      "        -6.04545972e-02,  7.06417408e-02, -3.13501069e-02,\n",
      "        -3.52731937e-02, -3.06330764e-02,  7.12445360e-02,\n",
      "        -2.72651108e-02],\n",
      "       [ 1.17764065e-01,  1.56140615e-01, -9.13879916e-02,\n",
      "         1.05120268e-01, -1.75378561e-02,  1.77321027e-02,\n",
      "        -7.45945131e-02, -8.60209242e-02,  1.75505864e-02,\n",
      "        -6.12695862e-02],\n",
      "       [ 1.04403428e-01,  2.21840849e-02,  1.74985586e-01,\n",
      "         8.69369927e-02,  1.47921901e-02,  2.00822823e-02,\n",
      "         4.73297515e-02,  1.51809481e-01,  1.63541492e-01,\n",
      "         2.38707301e-02],\n",
      "       [-9.12987644e-02,  1.95069309e-03,  1.11871438e-01,\n",
      "         3.73215449e-02, -3.17171598e-02,  3.08772253e-02,\n",
      "        -4.69054656e-04, -1.47464159e-01, -6.02484792e-02,\n",
      "         1.30601253e-02],\n",
      "       [ 1.02529815e-01,  2.39157064e-02,  8.99863811e-02,\n",
      "        -3.90303751e-02,  1.32129696e-01, -5.44865260e-02,\n",
      "         1.54773776e-02,  3.77128550e-02,  1.30275011e-02,\n",
      "        -5.51179969e-02],\n",
      "       [ 1.32879987e-01, -3.26277772e-02, -3.70367717e-03,\n",
      "        -4.54114191e-02,  6.40922469e-02,  5.07076907e-02,\n",
      "         7.73130876e-02,  1.25100524e-01, -9.60590113e-02,\n",
      "        -1.97845383e-02],\n",
      "       [-2.22283308e-01,  5.51380786e-02, -4.38987421e-02,\n",
      "        -4.00153802e-02,  9.79549677e-02, -4.61633298e-02,\n",
      "        -1.55935900e-01,  1.32148253e-01,  1.26821458e-02,\n",
      "        -2.24765633e-02],\n",
      "       [-4.80870950e-07, -5.53639044e-07,  2.19824079e-01,\n",
      "        -1.71353403e-10, -1.00133794e-01, -9.88759438e-08,\n",
      "        -2.17283068e-02, -3.26609034e-09,  6.96990573e-02,\n",
      "         1.97522142e-07],\n",
      "       [-2.36554463e-01,  7.52920314e-02, -1.50192269e-03,\n",
      "        -1.41829988e-02,  1.31872690e-01,  7.22252130e-02,\n",
      "        -2.22552769e-01,  6.32776568e-02,  3.11669501e-02,\n",
      "         1.29308490e-01],\n",
      "       [-1.59750563e-01,  5.12502671e-02,  5.83358651e-02,\n",
      "         1.36712188e-01,  1.02866318e-03,  1.03367738e-01,\n",
      "        -5.38322972e-02,  9.45703073e-02,  1.59094711e-02,\n",
      "         1.13007047e-01],\n",
      "       [-2.65077476e-02,  9.87474168e-02, -1.14162492e-02,\n",
      "         1.05087673e-01,  9.62005615e-02,  1.73042650e-01,\n",
      "         9.54089509e-02,  1.23237234e-02,  1.79023709e-01,\n",
      "         1.85868943e-01],\n",
      "       [ 7.55875042e-02,  1.16269453e-01,  5.11374356e-02,\n",
      "         1.19980870e-01,  1.54087611e-01,  8.96319886e-02,\n",
      "         1.70452450e-01,  6.04725389e-02,  6.47856764e-02,\n",
      "         8.32989182e-03],\n",
      "       [-2.85297797e-01,  5.91396284e-02, -7.86778879e-03,\n",
      "        -5.31937941e-02,  4.32268404e-02, -1.35099314e-01,\n",
      "        -7.68824350e-02,  4.45307476e-02, -6.90771170e-02,\n",
      "        -1.76268796e-02],\n",
      "       [-1.29311823e-02, -1.94221179e-01, -1.40303247e-02,\n",
      "        -4.54905686e-02, -7.72264205e-02,  9.41844853e-02,\n",
      "         9.96825404e-02, -1.35846209e-01,  3.01485740e-02,\n",
      "         1.13759558e-02],\n",
      "       [-1.50047215e-01, -1.63263620e-02,  1.13416106e-01,\n",
      "        -4.73550963e-02, -6.52346345e-02,  4.84437306e-02,\n",
      "         4.47430747e-02, -3.65785830e-02, -6.50610488e-02,\n",
      "         9.88157137e-02],\n",
      "       [-1.95284444e-02,  1.36682470e-01,  7.03741949e-02,\n",
      "         8.24877428e-02, -8.94359736e-02, -6.52836062e-02,\n",
      "        -4.57426086e-02,  4.34711560e-02, -3.09160586e-03,\n",
      "        -1.82261395e-02],\n",
      "       [ 1.53284631e-01, -6.05612392e-03, -1.23790253e-01,\n",
      "         4.42767734e-03, -1.44958150e-01, -6.85646620e-02,\n",
      "        -1.22710902e-01, -5.55435850e-02,  1.44293085e-01,\n",
      "        -1.04382948e-01],\n",
      "       [-9.82865665e-03,  3.87811508e-02,  2.31534222e-02,\n",
      "        -5.78531201e-02,  1.00978610e-01, -2.34294938e-02,\n",
      "        -3.65579974e-03,  9.24080154e-03,  4.14779974e-02,\n",
      "         4.05519043e-02],\n",
      "       [ 1.16300372e-02,  6.42061504e-02,  1.12533227e-01,\n",
      "         1.05932733e-01,  2.36299672e-02,  3.60394201e-02,\n",
      "         4.44645316e-02,  7.72272395e-02,  1.26885647e-01,\n",
      "         4.59554994e-02],\n",
      "       [-1.19049390e-02,  7.10098031e-03, -3.80386291e-02,\n",
      "         1.15695906e-01, -3.64004977e-02, -3.94240701e-02,\n",
      "        -2.88216698e-02,  1.55475564e-02,  8.35560406e-03,\n",
      "         4.02611879e-02],\n",
      "       [-1.77377943e-01, -4.97853366e-02, -2.01579962e-01,\n",
      "         8.17110207e-02, -7.15510823e-02,  3.87029252e-02,\n",
      "         1.14996596e-01, -2.62741197e-02, -1.16687156e-01,\n",
      "        -1.13281322e-01],\n",
      "       [-5.65525994e-02, -1.05556107e-01, -2.10142584e-02,\n",
      "        -4.69896515e-02, -7.41599887e-02, -3.73356093e-02,\n",
      "         8.43131176e-02, -2.50321270e-03, -1.12598618e-02,\n",
      "        -7.24240048e-02],\n",
      "       [-9.16044569e-02, -2.59318786e-02, -8.21109156e-02,\n",
      "        -9.79834707e-02,  1.22901089e-02, -2.35477693e-01,\n",
      "        -1.31245856e-01,  4.15408292e-02, -9.28880781e-02,\n",
      "        -5.01569353e-02],\n",
      "       [ 1.30051351e-01,  1.38880211e-01, -1.41532926e-02,\n",
      "        -7.08791274e-02,  3.70419519e-02,  1.63512489e-02,\n",
      "         1.24179125e-01,  5.08132834e-02,  6.20151703e-02,\n",
      "        -4.00107231e-02],\n",
      "       [-4.31716097e-02,  1.46208973e-01, -4.83662256e-02,\n",
      "        -2.69578794e-02,  2.99052390e-02,  1.99023943e-01,\n",
      "        -1.25080628e-02, -1.53979813e-01, -2.13112924e-03,\n",
      "         3.21524186e-02],\n",
      "       [-1.50155968e-01,  1.59149164e-01,  3.40120559e-02,\n",
      "         6.57767197e-02,  8.21918129e-02,  6.35058428e-04,\n",
      "         1.09062589e-01,  1.29664005e-01,  1.79771218e-02,\n",
      "         7.95552826e-03],\n",
      "       [ 1.10739160e-01,  4.72730905e-02,  9.19362242e-02,\n",
      "         8.08778344e-02,  1.30649598e-02,  1.56063919e-01,\n",
      "         1.50437961e-01,  6.36967010e-02,  1.50121497e-01,\n",
      "         7.38950792e-02],\n",
      "       [-7.38742853e-03, -1.69856469e-04,  6.28225815e-02,\n",
      "        -5.87385913e-03,  2.42586358e-02, -1.45236643e-03,\n",
      "        -3.86896602e-03, -5.90426203e-03, -2.37433350e-02,\n",
      "        -2.23023705e-02],\n",
      "       [ 8.85164762e-02, -5.25576575e-02,  8.06173070e-02,\n",
      "         3.46913844e-02,  1.50976218e-01,  9.60845009e-02,\n",
      "        -1.25963464e-01,  6.88125483e-02,  1.93510393e-02,\n",
      "         1.56669853e-01],\n",
      "       [-4.49043809e-02,  8.45159463e-02,  6.81177550e-02,\n",
      "         1.49461660e-01,  9.02140844e-02,  1.47084494e-01,\n",
      "        -1.11149281e-01,  4.46256047e-03,  1.31497764e-02,\n",
      "         1.41871089e-01],\n",
      "       [ 1.10599719e-01,  1.93194915e-01, -8.48830176e-02,\n",
      "        -1.00737828e-03,  5.12004799e-02,  1.12788105e-01,\n",
      "        -1.25652174e-01, -1.73602352e-02, -7.33461728e-03,\n",
      "        -2.21639158e-02],\n",
      "       [ 1.34163099e-01,  3.33135609e-02,  1.44905645e-01,\n",
      "         8.28412806e-02,  1.54308493e-01,  6.54207286e-02,\n",
      "         1.54532571e-01, -1.20332716e-01, -2.94721772e-03,\n",
      "        -1.19204621e-02],\n",
      "       [ 1.33936211e-01,  8.26145675e-02,  1.26484800e-02,\n",
      "         9.63018613e-02,  1.52725824e-01,  3.05346460e-02,\n",
      "         5.95519739e-02,  1.79342044e-01,  7.12495134e-02,\n",
      "         1.73213279e-01],\n",
      "       [-3.91687178e-02,  1.58921279e-01,  1.01748426e-01,\n",
      "        -1.25740943e-01,  4.51639648e-02,  3.09366351e-02,\n",
      "         1.99993166e-02,  2.01124487e-01, -5.14899643e-02,\n",
      "         1.76391587e-01],\n",
      "       [-6.35807090e-02,  3.52038318e-02,  1.50389990e-01,\n",
      "         1.38814339e-01, -1.58140771e-01,  2.44936884e-02,\n",
      "        -3.33212295e-02,  1.43609032e-01,  4.68922785e-02,\n",
      "        -1.25980973e-01],\n",
      "       [ 1.34727393e-01, -1.12329038e-01, -2.40567263e-02,\n",
      "         6.23053760e-02, -7.17393326e-02, -4.37682220e-03,\n",
      "         1.35955106e-01,  1.09696252e-01, -6.33034218e-03,\n",
      "         5.08945027e-02]])]\n"
     ]
    }
   ],
   "source": [
    "print(clf.coefs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clf.coefs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coefs_[0].shape\n",
    "#explain the dimension of the W matrix for the first (hidden) layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coefs_[1].shape\n",
    "#explain the dimension of the W matrix for the second (output) layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.intercepts_[0].shape\n",
    "#explain the dimension of the b (bias) vector for the first (hidden) layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.intercepts_[1].shape\n",
    "#explain the dimension of the b (bias) vector for the second (output) layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:46:05.771427Z",
     "start_time": "2018-11-13T15:46:05.279063Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9912166666666666"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:46:06.285050Z",
     "start_time": "2018-11-13T15:46:06.215576Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9646"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow( np.reshape( Xtest[120], (28,28) ) , cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5], dtype=uint8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(Xtest[120].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:46:46.642228Z",
     "start_time": "2018-11-13T15:46:46.615128Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyMLP:\n",
    "    def __init__(self, n_input, n_hidden, n_output, random_seed = 0, activate='sigmoid', verbose=False):\n",
    "        np.random.seed( random_seed )\n",
    "        # biases for hidden layer\n",
    "        self.b_hidden = np.random.randn( 1,n_hidden )\n",
    "        # weights for hidden layer\n",
    "        self.w_hidden = np.random.randn( n_input, n_hidden ) / np.sqrt(n_input+n_hidden)    # Glorot Initialization\n",
    "        # biases for output layer\n",
    "        self.b_output = np.random.randn( 1, n_output )\n",
    "        # weights for hidden layer\n",
    "        self.w_output = np.random.randn( n_hidden, n_output ) / np.sqrt(n_hidden+n_output)\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        if activate=='sigmoid':\n",
    "            self.activate = self.sigmoid\n",
    "            self.activate_der = self.sigmoid_der\n",
    "        else:\n",
    "            self.activate = self.tanh\n",
    "            self.activate_der = self.tanh_der\n",
    "\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "    def sigmoid_der(self,x):\n",
    "        g = 1.0/(1.0+np.exp(-x))\n",
    "        return g, g*(1.0-g)\n",
    "    \n",
    "    def tanh(self,x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def tanh_der(self,x):\n",
    "        g = np.tanh(x)\n",
    "        return g, 1.0-g**2\n",
    "\n",
    "    def softmax(self,x):\n",
    "        x -= np.max(x,axis=1,keepdims=True)\n",
    "        x  = np.exp(x)\n",
    "        x /= np.sum(x,axis=1,keepdims=True)\n",
    "        return x\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # X.shape = (m,n_input)\n",
    "        Z1 = X.dot(self.w_hidden) + self.b_hidden\n",
    "        # Z1.shape = (m,n_hidden)\n",
    "        A1 = self.activate( Z1 )\n",
    "        # A1.shape = (m,n_hidden)\n",
    "        Z2 = A1.dot(self.w_output) + self.b_output\n",
    "        # Z2.shape = (m,n_output)\n",
    "        A2 = self.softmax( Z2 )\n",
    "        # A2.shape = (m,n_output)\n",
    "        return A2\n",
    "    \n",
    "    def predict_class(self, X):\n",
    "        yhat = self.predict(X)\n",
    "        # pred.shape = (m,n_output)\n",
    "        # np.argmax( pred , axis=1 ).shape = (m,)\n",
    "        return np.argmax( yhat , axis=1 )\n",
    "    \n",
    "    def score(self,X,y):\n",
    "        return np.mean( self.predict_class(X) == y )\n",
    "    \n",
    "    # cross-entropy\n",
    "    def loss(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        return - np.mean( np.log( yhat[ range(len(yhat)), y ] ) )\n",
    "    \n",
    "    def fit(self, Xtrain, ytrain, epochs = 100, learning_rate = 0.1):\n",
    "        m,_ = Xtrain.shape\n",
    "        \n",
    "        for iter in range(epochs):  \n",
    "            # Forward propagation\n",
    "            # X.shape = (m,n_input)\n",
    "            Z1 = Xtrain.dot(self.w_hidden) + self.b_hidden\n",
    "            # Z1.shape = (m,n_hidden)\n",
    "            A1,dZ1 = self.activate_der( Z1 )\n",
    "            # A1.shape = (m,n_hidden)\n",
    "            Z2 = A1.dot(self.w_output) + self.b_output\n",
    "            # Z2.shape = (m,n_output)\n",
    "            A2 = self.softmax( Z2 )\n",
    "            # A2.shape = (m,n_output)\n",
    "\n",
    "            # Backward propagation\n",
    "#             delta2 = (A2-ytrain_one_hot)/m\n",
    "            delta2 = A2\n",
    "            delta2[ range(len(delta2)), ytrain ] -= 1\n",
    "            delta2 /= len(delta2)\n",
    "        \n",
    "            delta1 = delta2.dot( self.w_output.T ) * dZ1\n",
    "\n",
    "            dw_output = A1.T.dot(delta2)\n",
    "            dw_hidden = Xtrain.T.dot(delta1)\n",
    "\n",
    "            db_output = np.sum( delta2, axis=0, keepdims=True )\n",
    "            db_hidden = np.sum( delta1, axis=0, keepdims=True )\n",
    "            \n",
    "            \n",
    "            # Gradient descent\n",
    "            self.w_hidden -= (learning_rate * dw_hidden)\n",
    "            self.b_hidden -= (learning_rate * db_hidden)\n",
    "            self.w_output -= (learning_rate * dw_output)\n",
    "            self.b_output -= (learning_rate * db_output)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(\"Loss after iteration %i: %f (score=%.2f%%)\" %(iter, self.loss(Xtrain, ytrain), 100.0*self.score(Xtrain,ytrain)))\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:47:48.517951Z",
     "start_time": "2018-11-13T15:47:48.511334Z"
    }
   },
   "outputs": [],
   "source": [
    "mlp = MyMLP(784, 100, 10, random_seed=0, activate='tanh', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:00:47.505723Z",
     "start_time": "2018-11-13T15:57:33.640998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 2.019299 (score=28.32%)\n",
      "Loss after iteration 1: 1.805563 (score=46.39%)\n",
      "Loss after iteration 2: 1.469206 (score=57.15%)\n",
      "Loss after iteration 3: 1.224532 (score=64.34%)\n",
      "Loss after iteration 4: 1.136829 (score=66.25%)\n",
      "Loss after iteration 5: 1.052258 (score=69.16%)\n",
      "Loss after iteration 6: 0.975614 (score=71.74%)\n",
      "Loss after iteration 7: 0.885285 (score=75.40%)\n",
      "Loss after iteration 8: 0.826698 (score=77.34%)\n",
      "Loss after iteration 9: 0.772416 (score=79.48%)\n",
      "Loss after iteration 10: 0.730379 (score=80.32%)\n",
      "Loss after iteration 11: 0.706912 (score=81.41%)\n",
      "Loss after iteration 12: 0.682639 (score=81.25%)\n",
      "Loss after iteration 13: 0.656874 (score=83.03%)\n",
      "Loss after iteration 14: 0.637254 (score=82.59%)\n",
      "Loss after iteration 15: 0.620134 (score=83.98%)\n",
      "Loss after iteration 16: 0.600467 (score=83.79%)\n",
      "Loss after iteration 17: 0.585694 (score=84.99%)\n",
      "Loss after iteration 18: 0.571758 (score=84.61%)\n",
      "Loss after iteration 19: 0.557487 (score=85.81%)\n",
      "Loss after iteration 20: 0.542769 (score=85.40%)\n",
      "Loss after iteration 21: 0.535428 (score=86.39%)\n",
      "Loss after iteration 22: 0.520722 (score=86.11%)\n",
      "Loss after iteration 23: 0.512988 (score=86.95%)\n",
      "Loss after iteration 24: 0.498913 (score=86.75%)\n",
      "Loss after iteration 25: 0.490065 (score=87.51%)\n",
      "Loss after iteration 26: 0.479112 (score=87.47%)\n",
      "Loss after iteration 27: 0.471923 (score=88.00%)\n",
      "Loss after iteration 28: 0.468033 (score=87.72%)\n",
      "Loss after iteration 29: 0.461550 (score=88.13%)\n",
      "Loss after iteration 30: 0.453708 (score=88.01%)\n",
      "Loss after iteration 31: 0.446998 (score=88.47%)\n",
      "Loss after iteration 32: 0.440693 (score=88.56%)\n",
      "Loss after iteration 33: 0.435303 (score=88.78%)\n",
      "Loss after iteration 34: 0.428599 (score=88.80%)\n",
      "Loss after iteration 35: 0.424505 (score=88.98%)\n",
      "Loss after iteration 36: 0.419919 (score=89.16%)\n",
      "Loss after iteration 37: 0.418287 (score=89.19%)\n",
      "Loss after iteration 38: 0.412745 (score=89.29%)\n",
      "Loss after iteration 39: 0.409576 (score=89.38%)\n",
      "Loss after iteration 40: 0.404962 (score=89.40%)\n",
      "Loss after iteration 41: 0.397570 (score=89.66%)\n",
      "Loss after iteration 42: 0.394894 (score=89.62%)\n",
      "Loss after iteration 43: 0.390618 (score=89.77%)\n",
      "Loss after iteration 44: 0.388955 (score=89.85%)\n",
      "Loss after iteration 45: 0.385136 (score=89.91%)\n",
      "Loss after iteration 46: 0.382227 (score=89.95%)\n",
      "Loss after iteration 47: 0.378416 (score=90.09%)\n",
      "Loss after iteration 48: 0.373242 (score=90.27%)\n",
      "Loss after iteration 49: 0.371675 (score=90.17%)\n",
      "Loss after iteration 50: 0.369072 (score=90.20%)\n",
      "Loss after iteration 51: 0.366441 (score=90.29%)\n",
      "Loss after iteration 52: 0.364761 (score=90.20%)\n",
      "Loss after iteration 53: 0.360505 (score=90.52%)\n",
      "Loss after iteration 54: 0.356976 (score=90.55%)\n",
      "Loss after iteration 55: 0.354501 (score=90.68%)\n",
      "Loss after iteration 56: 0.350281 (score=90.73%)\n",
      "Loss after iteration 57: 0.347978 (score=90.95%)\n",
      "Loss after iteration 58: 0.346722 (score=90.79%)\n",
      "Loss after iteration 59: 0.346387 (score=90.75%)\n",
      "Loss after iteration 60: 0.342827 (score=90.99%)\n",
      "Loss after iteration 61: 0.339656 (score=91.05%)\n",
      "Loss after iteration 62: 0.337324 (score=91.01%)\n",
      "Loss after iteration 63: 0.335930 (score=91.18%)\n",
      "Loss after iteration 64: 0.333920 (score=91.11%)\n",
      "Loss after iteration 65: 0.333390 (score=91.18%)\n",
      "Loss after iteration 66: 0.330326 (score=91.16%)\n",
      "Loss after iteration 67: 0.329840 (score=91.22%)\n",
      "Loss after iteration 68: 0.326164 (score=91.31%)\n",
      "Loss after iteration 69: 0.328033 (score=91.25%)\n",
      "Loss after iteration 70: 0.326048 (score=91.12%)\n",
      "Loss after iteration 71: 0.323941 (score=91.39%)\n",
      "Loss after iteration 72: 0.323184 (score=91.26%)\n",
      "Loss after iteration 73: 0.320744 (score=91.43%)\n",
      "Loss after iteration 74: 0.318890 (score=91.44%)\n",
      "Loss after iteration 75: 0.317076 (score=91.45%)\n",
      "Loss after iteration 76: 0.315001 (score=91.56%)\n",
      "Loss after iteration 77: 0.312612 (score=91.62%)\n",
      "Loss after iteration 78: 0.309926 (score=91.67%)\n",
      "Loss after iteration 79: 0.309183 (score=91.67%)\n",
      "Loss after iteration 80: 0.307569 (score=91.77%)\n",
      "Loss after iteration 81: 0.307749 (score=91.69%)\n",
      "Loss after iteration 82: 0.308788 (score=91.64%)\n",
      "Loss after iteration 83: 0.306288 (score=91.61%)\n",
      "Loss after iteration 84: 0.305308 (score=91.81%)\n",
      "Loss after iteration 85: 0.303962 (score=91.77%)\n",
      "Loss after iteration 86: 0.302008 (score=91.87%)\n",
      "Loss after iteration 87: 0.300958 (score=91.81%)\n",
      "Loss after iteration 88: 0.300336 (score=91.86%)\n",
      "Loss after iteration 89: 0.301229 (score=91.72%)\n",
      "Loss after iteration 90: 0.297508 (score=91.91%)\n",
      "Loss after iteration 91: 0.293893 (score=92.05%)\n",
      "Loss after iteration 92: 0.290218 (score=92.14%)\n",
      "Loss after iteration 93: 0.287007 (score=92.31%)\n",
      "Loss after iteration 94: 0.288154 (score=92.25%)\n",
      "Loss after iteration 95: 0.284542 (score=92.26%)\n",
      "Loss after iteration 96: 0.283862 (score=92.36%)\n",
      "Loss after iteration 97: 0.281894 (score=92.45%)\n",
      "Loss after iteration 98: 0.281573 (score=92.36%)\n",
      "Loss after iteration 99: 0.278317 (score=92.55%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit( Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:00:47.585501Z",
     "start_time": "2018-11-13T16:00:47.508352Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9176"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.score( Xtest, ytest )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "Load MNIST fashion dataset, it has the same dimensions as MNIST dataset <br/>\n",
    "https://keras.io/api/datasets/fashion_mnist/  <br/> \n",
    "Go to the sklearn MLPClassifier website, learn about parameters and try different ones to achieve the \n",
    "best accuracy on the test set <br/>\n",
    "Comment on bias (overfitting) and variance (underfitting) <br/>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fashion_mnist.load_data()\n",
    "\n",
    "(Xtrain, ytrain), (Xtest, ytest) = dataset\n",
    "\n",
    "n_train = len(Xtrain)\n",
    "n_test = len(Xtest)\n",
    "\n",
    "n_features = 28*28\n",
    "\n",
    "Xtrain = Xtrain.reshape( n_train, n_features )\n",
    "Xtest  = Xtest.reshape( n_test, n_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow( np.reshape( Xtrain[5000], (28,28) ) , cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100,50,25,10), max_iter=50,activation = 'tanh',\n",
    "                    solver='adam',random_state=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.18933755\n",
      "Iteration 2, loss = 0.83997346\n",
      "Iteration 3, loss = 0.80187300\n",
      "Iteration 4, loss = 0.76066321\n",
      "Iteration 5, loss = 0.76789468\n",
      "Iteration 6, loss = 0.73175515\n",
      "Iteration 7, loss = 0.74864025\n",
      "Iteration 8, loss = 0.74684676\n",
      "Iteration 9, loss = 0.76970043\n",
      "Iteration 10, loss = 0.73019225\n",
      "Iteration 11, loss = 0.69392482\n",
      "Iteration 12, loss = 0.71517872\n",
      "Iteration 13, loss = 0.71591268\n",
      "Iteration 14, loss = 0.71654923\n",
      "Iteration 15, loss = 0.67798672\n",
      "Iteration 16, loss = 0.68589243\n",
      "Iteration 17, loss = 0.67594368\n",
      "Iteration 18, loss = 0.67245901\n",
      "Iteration 19, loss = 0.65813810\n",
      "Iteration 20, loss = 0.66105805\n",
      "Iteration 21, loss = 0.68405609\n",
      "Iteration 22, loss = 0.67209643\n",
      "Iteration 23, loss = 0.64908035\n",
      "Iteration 24, loss = 0.66023584\n",
      "Iteration 25, loss = 0.70868791\n",
      "Iteration 26, loss = 0.66146557\n",
      "Iteration 27, loss = 0.65198242\n",
      "Iteration 28, loss = 0.68577464\n",
      "Iteration 29, loss = 0.64554093\n",
      "Iteration 30, loss = 0.66151554\n",
      "Iteration 31, loss = 0.66002791\n",
      "Iteration 32, loss = 0.64500416\n",
      "Iteration 33, loss = 0.62230757\n",
      "Iteration 34, loss = 0.62621696\n",
      "Iteration 35, loss = 0.65174395\n",
      "Iteration 36, loss = 0.62078922\n",
      "Iteration 37, loss = 0.60595106\n",
      "Iteration 38, loss = 0.63096458\n",
      "Iteration 39, loss = 0.68141874\n",
      "Iteration 40, loss = 0.67204285\n",
      "Iteration 41, loss = 0.64080585\n",
      "Iteration 42, loss = 0.63890526\n",
      "Iteration 43, loss = 0.61499496\n",
      "Iteration 44, loss = 0.61012985\n",
      "Iteration 45, loss = 0.61728782\n",
      "Iteration 46, loss = 0.63242710\n",
      "Iteration 47, loss = 0.63054816\n",
      "Iteration 48, loss = 0.62296418\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', hidden_layer_sizes=(100, 50, 25, 10),\n",
       "              max_iter=50, random_state=1, verbose=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7175666666666667"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(Xtest, ytest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
